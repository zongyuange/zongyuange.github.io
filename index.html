<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Zongyuan Ge Homepage</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Zongyuan Ge</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="papers.html">Papers</a></div>
<!-- <div class="menu-item"><a href="tutorials.html">Tutorials</a></div>
 --><div class="menu-item"><a href="awards.html">Awards and Grants</a></div>
<!-- <div class="menu-category">Research</div> -->
<!-- <div class="menu-item"><a href="books.html">Books</a></div>
 -->
 <!-- <div class="menu-item"><a href="papers.html">Papers</a></div> -->
<!-- <div class="menu-item"><a href="projects.html">Projects</a></div>
 -->
<!-- <div class="menu-item"><a href="tutorials.html">Tutorials</a></div> -->
<!-- <div class="menu-item"><a href="workshops.html">Workshops</a></div>
 -->
<!--  <div class="menu-item"><a href="talks.html">Invited&nbsp;talks</a></div>
 -->
<!-- <div class="menu-category">Awards &amp; Honors</div> -->
<!-- <div class="menu-item"><a href="awards.html">Grants and Awards</a></div> -->
<!-- <div class="menu-item"><a href="competitions.html">Competitions</a></div>
 -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Zongyuan Ge（戈宗元）</h1>
</div>
<table class="imgtable"><tr>

<!-- <td align="centre">
<a href="https://www.monash.edu/mmai-group/"><img src="imgs/MMAI.png" alt=“Monash” width="230px" height="110px" /></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</td> -->

<td align="left">
<img class="mycss" src="imgs/IMG_2961.JPG" alt="gzy" width="260px" height="260px" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<!-- <td> -->
	<br/>
	<strong>Zongyuan Ge</strong><br />
	<strong>Ph.D., Associate Professor</strong><br />
	Faculty of IT,<br /> 
	Faculty of Engineering,<br />
	Monash University,<br />
	Office: G34, 15 Innovation Walk<br />
	Email: zongyuan.ge _at_ monash.edu <strong>or</strong> z.ge _at_ outlook.com &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<br /><br />
	<a href="https://scholar.google.com.au/citations?user=Q0gUrcIAAAAJ&hl=en"><img src="imgs/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>&nbsp;&nbsp;
	<a href=""><img src="imgs/zhihu.jpg" height="30px" style="margin-bottom:-3px"></a>&nbsp;&nbsp;
	<a href="https://www.linkedin.com/in/zongyuange/"><img src="imgs/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>&nbsp;&nbsp;
	<a href="https://orcid.org/0000-0002-5880-8673"><img src="imgs/ORCID.jpg" height="30px" style="margin-bottom:-3px"></a>
	<br />

<!-- </td> -->

<!-- <td>
<a href="https://www.monash.edu/"><img src="imgs/monash.png" alt=“Monash” width="250px" height="95px" /></a>
</td> -->

<td>
<a href="https://www.monash.edu/mmai-group/"><img src="imgs/MMAI.png" alt=“Monash” width="240px" height="110px" /></a>
</td>

<!-- <td align="left">
</td> -->
</td>
</tr></table>
<br />
<div id="content-background">
<h2>About Me</h2>
<ul>
<p> I currently hold the tenured position of Associate Professor at the Monash University Faculty of IT, along with honorary and adjunct positions as Research Affiliate at the Australian Centre for Robotic Vision, NVIDIA-AI Fellow, the Chief Scientist at Monash-Airdoc Research Centre and Chief Research Officer at Eyetelligence. 
</li>
</ul>
<ul>

<p> 
After returning to academia in 2018, I founded the <a href="https://www.monash.edu/mmai-group">Monash Medical AI Group</a>, which targets for advancing medical AI research and deployment. I have brought more than 25 million dollars to the group, and it is now funding over fully-funded 20+ PhD students, 6 Research Fellows/Scientists, and several Research Master/FYP students. </p>
</li>
</ul>
<ul>


<!-- <li><p>I conduct interdisciplinary research at the boundary between artificial intelligence, computer-aided diagnosis, biomedical engineering, medical imaging and machine learning and is a multi-award winning medical information science and technology entrepreneur. My research leverages cutting-edge AI technologies using large-scale multi-modality medical data including imaging, medical records, gene data and models the clinicians’ medical knowledge underlying tasks like diagnosis and prognosis for eye (ophthalmology), skin (dermatology), heart (cardiovascular) and neurodegeneration diseases.</p>
</li>
</ul>
<ul> -->

<p style="color:rgb(255,0,0);">[Call for 2023] Please contact me through my email if you are interested in applying for scientist, software engineer, machine learning engineer, visiting scholar positions in my lab.</p></p>
</li>
</ul>
</div>

<!-- <h2>Call for Papers</h2>
<ul>
<li><p><b>Pattern Recognition Special Issue on <a href="https://www.journals.elsevier.com/pattern-recognition/call-for-papers/fine-grained-object-retrieval-matching-and-ranking">Fine-Grained Object Retrieval, Matching and Ranking</a>&ndash;Before Oct. 10, 2021.</b></p></p>
</li>
</ul>
<h2>Updates</h2>
<ul>
<li><p>Eg: Jun. 21, 2021: Very honored to serve as an Area Chair in <a href="https://www.bmvc2021.com/">BMVC 2021</a>.</p>
</li>
</ul> -->



<!-- <h2>Research Interests</h2>
<p>My research interests include some sub-fields of <b>Computer Vision</b> and <b>Machine Learning</b>:</p>
<ul>
<li><p><b>Deep Convolutional Neural Networks</b> (DCNN) is a type of feed-forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field, which is widely used in image and video related tasks.</p>
</li>
</ul> -->


<h2>Research Interests</h2>
<p>I conduct interdisciplinary research at the boundary between medical artificial intelligence, computer-aided diagnosis, biomedical engineering, medical imaging and machine learning and is a multi-award-winning medical information science and technology entrepreneur. My research leverages cutting-edge AI technologies using large-scale multi-modality medical data including imaging, medical records, gene data and models the clinicians’ medical knowledge underlying tasks like diagnosis, prognosis, and treatment for eye (ophthalmology), skin (dermatology), heart (cardiovascular) and neurodegeneration diseases such as epilepsy and multiple sclerosis. I am also one of Australia’s most in-demand experts in technology, including medical robotics and artificial intelligence, and is a passionate science communicator. </p>
<br><br> As a lifelong medical technology entrepreneur, I have led and contributed to several international research projects in the areas of dermatology, ophthalmology, radiology, and neurology with major industry companies like IBM Watson Health, medical technology unicorn company Airdoc/Eyetelligence and medical/healthcare services providers such as Molemap Clinic, The Alfred Health, Royal Melbourne Hospital, and Princess Alexandra Hospital. I serve as the Chief Research Officer and launched the company Eyetelligence (https://eyetelligence.ai/au/), which targets the massive global optometry and optical dispensing market. Eyetelligence has received funding of over 25 million dollars from various venture capital, family foundation and the Morrison Government. My company has developed the Eyeteligence workstation, which can screen for the four leading causes of blindness in adults: DR, Cataracts, AMD, and Glaucoma and sold to all 48 Australian BUPA Optical stores and all Specsavers 400 Australia and NZ stores. I am also the Chief Scientist for the Airdoc-Airdoc Research Centre (https://www.airdoc.com/) to lead the AI technology and product development team. In 2019, he helped build the Monash-Airdoc Research Centre (with 3.5m total investment) to help developed CFDA, CLASS III approved healthcare monitoring products. This eye disease diagnosis AI product has served millions of people in China, India, Japan, South Africa and Australia. According to the financial report, the revenue generated from the ophthalmology AI products A/Prof Ge’s team co-developed has generated 25 million dollars in revenue for the company. Airdoc has been successfully listed on Hongkong Stock Exchange (2251-HK) with a market cap of over 1 billion US dollars (2021, Dec).
<ul>
</ul>


<div id="content-background">
<h2>Recent Selected Publications</h2>
<ul>


<li><p><a href="https://jamanetwork.com/journals/jamaneurology/article-abstract/2795867">Development and validation of a deep learning model for predicting treatment response in patients with newly diagnosed epilepsy
</a>
<br /><i>JAMA neurology 79.10: 986-996.</i>, 2022</p>
</li>

<li><p><a href="https://www.nature.com/articles/s41433-022-02239-4">Artificial intelligence to distinguish retinal vein occlusion patients using color fundus photographs
</a>
<br /><i>Nature Eye (2022): 1-7.</i>, 2022</p>
</li>

<li><a href="https://scholar.google.com.au/citations?view_op=view_citation&amp;hl=en&amp;user=Q0gUrcIAAAAJ&amp;cstart=20&amp;pagesize=80&amp;citation_for_view=Q0gUrcIAAAAJ:8AbLer7MMksC" class="gsc_a_at">Application of Comprehensive Artificial intelligence Retinal Expert (CARE) system: a national real-world evidence study</a>
<!-- <br /> D Lin, J Xiong, C Liu, L Zhao, Z Li, S Yu, X Wu, <STRONG>Z Ge</STRONG>, X Hu, B Wang, M Fu, ...<br /><em> -->
<br />The Lancet Digital Health 3 (8), e486-e495<span class="gs_oph">, 2021</span></em>
<br /></li>

<li><p><a href="https://arxiv.org/pdf/2011.13816.pdf">Leveraging Regular Fundus Images for Training UWF Fundus Diagnosis Models via Adversarial Learning and Pseudo-Labeling</a>.
<!-- <br />L.Ju, X.Wang, Z.Xin, P.Bonnington, T.Drummond, <b>Z.Ge</b> -->
<br /><i>IEEE Transaction on Medical Imaging (TMI)</i>, 2021</p>
</li>

<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/fc146be0b230d7e0a92e66a6114b840d-Paper.pdf">Hierarchical Neural Architecture Search for Deep Stereo Matching–Supplementary Materials
</a>
<!--<br />X.Cheng, Y.Zhong, M.Harandi, Y.Dai, X.Chang, H.Li, T.Drummond, <b>Z.Ge</b>-->
<br /><i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2021</p>
</li>

<li><p><a href="https://www.bmj.com/content/371/bmj.m3658.long">New era of personalised epilepsy management
</a>
<!--<br />Z.Chen, B.Rollo, A.Baker, A.Anderson, Y.Ma, T.Obrien, <b>Z.Ge</b>, X.Wang, P.Kwan-->
<br /><i>British Medical Journal (The BMJ)</i>, 2021</p>
</li>


<li><p><a href="https://openreview.net/pdf?id=Eql5b1_hTE4">Robust early-learning: Hindering the memorization of noisy labels. In International Conference on Learning Representations</a>.
<!--<br />Xia, X., Liu, T., Han, B., Gong, C., Wang, N., <b>Z.Ge</b>, Chang, Y.-->
<br /><i>International Conference on Learning Representations (ICLR)</i>, 2020</p>
</li>


<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9247292">One-Shot Neural Architecture Search: Maximising Diversity to Overcome Catastrophic Forgetting
</a>
<!--<br />M.Zhang, H.Li, S.Pan, X.Chang, <b>Z.Ge</b>, S.Su-->
<br /><i>IEEE Transactions on Pattern Analysis and Machine Intelligence
 (TPAMI)</i>, 2020</p>
</li>



<!-- <li><p><a href="publication/tip17SCDA.pdf">Robust early-learning: Hindering the memorization of noisy labels. In International Conference on Learning Representations</a>.
<br /><p style="color:red"><i>(This work was the highly cited paper by ESI, which received enough citations to place it in the top 1% of its academic field.)</i></p>
<b>X.-S. Wei</b>, J.-H. Luo, J. Wu, and Z.-H. Zhou.
<br /><i>IEEE Transactions on Image Processing (TIP)</i>, 2017, 26(6): 2868-2881.</p>
</li>
 -->
</ul>
</div>

<h2>Professional Activities (Selected)</h2>
<h3>Professional Memberships and Service</h3>
<ul>

<li><p>Scientific Advisory Committee, <a href="https://ardc.edu.au/">The Australian Research Data Commons (ARDC)</a>, 2020 -</p>
</li>
<li><p>Steering Group member, <a href="https://www.monash.edu/researchinfrastructure/datascienceandai/home">Monash Data Science and AI platform</a>, 2020 -</p>
</li>
<li><p>Research Advisory Committee,  <a href="https://acemid.centre.uq.edu.au/">Australian Centre of Excellence Melanoma Imaging Diagnosis (ACEMID)</a>, 2020 -</p>
</li>
<li><p>Member at Large, <a href="http://aprs.dictaconference.org/">Australian Pattern Recognition Society (APRS)</a>, 2020 -</p>
</li>
<li><p>Panel Assessors, <a href="https://www.monash.edu/medicine/ccs">Monash University Central Clinical School, Faculty of Medicine</a>, 2019 -</p>
</li>


<!-- </ul>
<h3>Program Chair / Co-Chair</h3>
<ul>
<li><p>ACM International Conference on Multimedia (ACM MM) Workshop on <a href="https://ltdl-ijcai21.github.io/">Multimedia Understanding with Less Labeling</a>, 2021</p>
</li>
<li><p>International Joint Conference on Artificial Intelligence (IJCAI) Workshop on <a href="https://ltdl-ijcai21.github.io/">Long-Tailed Distribution Learning</a>, 2021</p>
</li>
<li><p>International Conference on Computer Vision (ICCV) Workshop on <a href="http://chalearnlap.cvc.uab.es/workshop/44/description/">Understanding Social Behavior in Small Group Interactions</a>, 2021</p>
</li>
<li><p>Asian Conference on Computer Vision (ACCV) Workshop on <a href="https://sites.google.com/view/webfg2020">Webly-Supervised Fine-Grained Recognition</a>, 2020</p>
</li>
</ul>
<h3>Publicity Chair</h3>
<ul>
<li><p>PRCV <a href="http://www.prcv.cn/committees_en.html">2020</a></p>
</li>
</ul>
<h3>Guest Editor</h3>
<ul>
<li><p>Pattern Recognition Special Issue on <a href="https://www.journals.elsevier.com/pattern-recognition/call-for-papers/fine-grained-object-retrieval-matching-and-ranking">Fine-Grained Object Retrieval, Matching and Ranking</a>, 2021</p>
</li>
</ul>
<h3>Area Chair / Senior PC</h3>
<ul>
<li><p>IJCAI <a href="https://ijcai-21.org/">2021</a></p>
</li>
<li><p>BMVC <a href="https://www.bmvc2021.com/">2021</a></p>
</li> -->
</ul>
<h3>Program Committee Member</h3>
<ul>
<li><p>CVPR <a href="http://cvpr2018.thecvf.com/">2018</a>, <a href="http://cvpr2019.thecvf.com/">2019</a>, <a href="http://cvpr2020.thecvf.com/">2020</a>, <a href="http://cvpr2021.thecvf.com/">2021</a>, <a href="http://cvpr2022.thecvf.com/">2022</a>, <a href="http://cvpr2023.thecvf.com/">2023</a></p>
</li>
<li><p>MICCAI <a href="https://www.miccai2019.org/">2019</a>, <a href="https://miccai2020.org/en/">2020</a>, <a href="https://www.miccai2021.org/en/">2021</a>, <a href="https://www.miccai2021.org/en/">2022</a>, <a href="https://www.miccai2021.org/en/">2022</a></p>
<li><p>ICCV <a href="http://iccv2019.thecvf.com/">2019</a>, <a href="http://iccv2021.thecvf.com/home">2021</a></p>, <a href="http://iccv2023.thecvf.com/home">2023</a></p>
<li><p>IJCAI <a href="https://www.ijcai-18.org/">2018</a>, <a href="https://ijcai19.org/">2019</a>,  <a href="https://www.ijcai20.org/">2020</a>,  <a href="https://www.ijcai21.org/">2021</a>,  <a href="https://www.ijcai22.org/">2022</a></p>
</li>
<li><p>AAAI <a href="https://aaai.org/Conferences/AAAI-19/">2019</a>, <a href="https://aaai.org/Conferences/AAAI-20/">2020</a>, <a href="https://aaai.org/Conferences/AAAI-21/">2021</a>, <a href="https://aaai.org/Conferences/AAAI-22/">2022</a></p>
<li><p>WACV <a href="http://wacv2021.thecvf.com/">2018</a>, <a href="http://wacv2021.thecvf.com/">2019</a>, <a href="https://wacv20.wacv.net/">2020</a>, <a href="http://wacv2021.thecvf.com/">2021</a></p>
</li>

</p>
</li>
</ul>
<h3>Journal Reviewer</h3>
<ul>
<li><p>IEEE Transactions on Medical Imaging (TMI)</p>
</li>
<li><p>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</p>
</li>
<li><p>International Journal of Computer Vision (IJCV)</p>
</li>
<li><p>IEEE Transactions on Image Processing (TIP)</p>
</li>
<li><p>Elsevier Journal of Medical Image Analysis (MIA)</p>
</li>
<li><p>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</p>
</li>
<li><p>IEEE Transactions on Multimedia (TMM)</p>
</li>
<li><p>Elsevier Journal of Neural Networks (NN)</p>
</li>
<li><p>Elsevier Journal of Pattern Recognition (PR)
</p>
</li>


</p>
</li>
</ul>


<div id="content-background">
<h2>Correspondence</h2>
<h3>Office</h3>
<p>eResearch Centre, 15 Innovation Walk, Monash University</p>
<h3>Tel</h3>
<p>+61 (03) 9905 9916
<br /><br /></p>
</div>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=DkX2f_8RZ03wlQUJzl9vnrCtQIN5wcwJuIzyF4OasZY'></script>
<div id="footer">
<div id="footer-text">
&copy; Zongyuan Ge  | Last updated: Dec 2022</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
